---
title: Simple explanation of Quasi-Newton methods
date: 2025-10-25 00:00:00 +0300
categories: [Optimization, Quasi-Newton]
tags: [optimization, quasi-newton, sr1]     # TAG names should always be lowercase
description: Simple explanation of the secant equation and how quasi-newton methods use it.
math: true
---

## Preface

At time step {::nomarkdown}$t${:/nomarkdown} the parameters will be denoted as {::nomarkdown}$\mathbf{x}_t${:/nomarkdown}; gradient at those parameters as {::nomarkdown}$\mathbf{g}(\mathbf{x}_t)${:/nomarkdown} and Hessian as {::nomarkdown}$H(\mathbf{x}_t)${:/nomarkdown}.

For this article I assume the reader knows what gradient and Hessian are, and is familiar with Newton's method for optimization, which in the most basic form performs the following update:
{::nomarkdown}$$
\mathbf{x}_{t+1} \leftarrow \mathbf{x}_t - H(\mathbf{x}_t)^{-1}\mathbf{g}(\mathbf{x}_t)
$${:/nomarkdown}

## Introduction

Quasi-newton methods approximate the Hessian or Hessian inverse using only gradients. Let's see how they do that!

## Secant equation

Secant equation is a core concept in Quasi-Newton methods.

Suppose we have Hessian {::nomarkdown}$H${:/nomarkdown}, some vector {::nomarkdown}$\mathbf{s}${:/nomarkdown}, and we compute a Hessian-vector product with it and call it {::nomarkdown}$\mathbf{y}${:/nomarkdown}. So we get:
{::nomarkdown}$$
H\mathbf{s} = \mathbf{y}
$${:/nomarkdown}
This is the secant equation. If we had {::nomarkdown}$\mathbf{s}${:/nomarkdown} and {::nomarkdown}$\mathbf{y}${:/nomarkdown}, we would know that the Hessian must satisfy {::nomarkdown}$H\mathbf{s} = \mathbf{y}${:/nomarkdown}.

It might seem that in order to obtain {::nomarkdown}$\mathbf{y}${:/nomarkdown}, one needs the Hessian to compute {::nomarkdown}$H\mathbf{s}${:/nomarkdown}. But there is a simple way to get {::nomarkdown}$\mathbf{s}${:/nomarkdown} and {::nomarkdown}$\mathbf{y}${:/nomarkdown} basically for free without knowing the Hessian.

## Finite-difference Hessian-vector products

If we compute difference between consequtive parameters and assign it to {::nomarkdown}$\mathbf{s}_t${:/nomarkdown}, and assign difference between consequtive gradients to {::nomarkdown}$\mathbf{y}_t${:/nomarkdown}, then {::nomarkdown}$\mathbf{y}_t${:/nomarkdown} approximates Hessian-vector product with {::nomarkdown}$\mathbf{s}_t${:/nomarkdown}:
{::nomarkdown}$$
\mathbf{s}_t = \mathbf{x}_t - \mathbf{x}_{t-1}
$${:/nomarkdown}
{::nomarkdown}$$
\mathbf{y}_t = \mathbf{g}(\mathbf{x}_t) - \mathbf{g}(\mathbf{x}_{t-1})
$${:/nomarkdown}
{::nomarkdown}$$
\text{then: } H(\mathbf{x}_t) \mathbf{s_t} \approx \mathbf{y}_t
$${:/nomarkdown}

To understand why, we can look at the formula of a [finite-differerence approximation to a Hessian-vector product](https://justindomke.wordpress.com/2009/01/17/Hessian-vector-products/).

 A Hessian-vector product, such as {::nomarkdown}$H(\mathbf{x})\mathbf{s}${:/nomarkdown}, can be approximated using this (backward finite difference) formula:
{::nomarkdown}$$
H(\mathbf{x})\mathbf{s} \approx \frac{\mathbf{g}(\mathbf{x}) - \mathbf{g}(\mathbf{x} - r\mathbf{s})}{r}
$${:/nomarkdown}

Here {::nomarkdown}$\mathbf{x}${:/nomarkdown} is the parameters where Hessian-vector product is approximated, and {::nomarkdown}$r${:/nomarkdown} is a small constant. With infinite precision the approximation becomes exact in the limit {::nomarkdown}$r \rightarrow 0${:/nomarkdown}.

Now let's set {::nomarkdown}$r${:/nomarkdown} to 1 and replace {::nomarkdown}$\mathbf{s}${:/nomarkdown} with {::nomarkdown}$\mathbf{x}_t - \mathbf{x}_{t-1}${:/nomarkdown}:
{::nomarkdown}$$
H(\mathbf{x}_t) \mathbf{s}_t \approx \frac{\mathbf{g}(\mathbf{x}_t) - \mathbf{g}(\mathbf{x}_t - 1 (\mathbf{x}_t - \mathbf{x}_{t-1}))}{1} = \mathbf{g}(\mathbf{x}_t) - \mathbf{g}(\mathbf{x}_{t-1}) = \mathbf{y}_t
$${:/nomarkdown}
so:
{::nomarkdown}$$
H(\mathbf{x}_t) \mathbf{s}_t \approx \mathbf{y}_t
$${:/nomarkdown}

Of course by setting {::nomarkdown}$r${:/nomarkdown} to 1, it seems that the approximation would be crude, but in practice it works exceptionally well in Quasi-Newton methods.

## Least-change principle

Quasi-Newton methods maintain a hessian approximation {::nomarkdown}$B${:/nomarkdown} (or it's inverse) and refine it on each iteration.

Suppose we have current Hessian approximation {::nomarkdown}$B_{t-1}${:/nomarkdown}, and we obtain {::nomarkdown}$\mathbf{s}_t = \mathbf{x}_t - \mathbf{x}_{t-1}${:/nomarkdown} and {::nomarkdown}$\mathbf{y}_t = \mathbf{g}(\mathbf{x}_t) - \mathbf{g}(\mathbf{x}_{t-1})${:/nomarkdown}. We know that {::nomarkdown}$H(\mathbf{x}_t) \mathbf{s}_t \approx \mathbf{y}_t${:/nomarkdown}, therefore we want our new refined hessian approximation {::nomarkdown}$B_t${:/nomarkdown} to satisfy {::nomarkdown}$B_t \mathbf{s}_t = \mathbf{y}_t${:/nomarkdown}. However on it's own this isn't too useful as there are infinitely many possible {::nomarkdown}$B_t${:/nomarkdown} that satisfy {::nomarkdown}$B_t \mathbf{s}_t = \mathbf{y}_t${:/nomarkdown}.

The idea of the least-change principle is to pick {::nomarkdown}$B_t${:/nomarkdown} such that it is close to {::nomarkdown}$B_{t-1}${:/nomarkdown}, preferably as close as possible. Since {::nomarkdown}$B_{t-1}${:/nomarkdown} might've accumulated a lot of curvature information from previous iterations, we don't want to throw that information away - keeping {::nomarkdown}$B_t${:/nomarkdown} close to {::nomarkdown}$B_{t-1}${:/nomarkdown} makes sure as little of that information is lost as possible. "Close" can be interpreted in many different ways, leading to different Quasi-Newton formulas.

Least-change principle is a heuristic (to my knowledge), but it works exceptionally well in practice and converges to true Hessian. There are multiple interpretations of why the it works, for example:

-
    *Lukšan, L., & Spedicato, E. (2000). Variable metric methods for unconstrained optimization and nonlinear least squares. Journal of Computational and Applied Mathematics, 124(1-2), 61-95.*:
    >Roughly speaking, the least-change principle guarantees that as much information from previous iterations as possible is saved while the quasi-Newton condition brings new information because it is satisfied by matrix.

-
    *[Greenstadt, J. (1970). Variations on variable-metric methods.(With discussion). Mathematics of Computation, 24(109), 1-22.](https://web.archive.org/web/20170817130143id_/https://www.ams.org/journals/mcom/1970-24-109/S0025-5718-1970-0258248-4/S0025-5718-1970-0258248-4.pdf)*:
    > Let us ask for the “best” correction in some sense. There are many possible choices to make, but a good one is to ask for the smallest correction, in the sense of some norm. To a certain extent, this would tend to keep the elements of [{::nomarkdown}$B^{−1}_k${:/nomarkdown} ] from growing too large, which might cause an undesirable instability.

## Deriving a Quasi-Newton method

Now let's derive a Quasi-Newton method. We will derive the Symmetric Rank 1 (SR1) method because it is simple.

What we have:

- current Hessian approximation {::nomarkdown}$B_{t-1}${:/nomarkdown};
- {::nomarkdown}$\mathbf{s}_t = \mathbf{x}_t - \mathbf{x}_{t-1}${:/nomarkdown}
- {::nomarkdown}$\mathbf{y}_t = \mathbf{g}(\mathbf{x}_t) - \mathbf{g}(\mathbf{x}_{t-1})${:/nomarkdown};

What we want:

- {::nomarkdown}$B_t${:/nomarkdown} that satisfies the secant equation {::nomarkdown}$B_t\mathbf{s}_t=\mathbf{y}_t${:/nomarkdown}
- we want {::nomarkdown}$B_t${:/nomarkdown} to be close to {::nomarkdown}$B_{t-1}${:/nomarkdown} in some sense, so that little information is lost from {::nomarkdown}$B_t${:/nomarkdown}. For SR1 we find the lowest rank update to {::nomarkdown}$B_{t-1}${:/nomarkdown}, which is rank-1 and is unique.
- there are other useful constraints one can impose on {::nomarkdown}$B_t${:/nomarkdown}:
  - usually {::nomarkdown}$B_t${:/nomarkdown} is constrained to be symmetric: {::nomarkdown}$B_t=B_t^\top${:/nomarkdown}.
  - {::nomarkdown}$B_t${:/nomarkdown} is often constrained to be positive-definite, however the SR1 formula does not do that.

A simple way to keep {::nomarkdown}$B_t${:/nomarkdown} symmetric and not too far from {::nomarkdown}$B_{t-1}${:/nomarkdown} is to perform a symmetric rank-1 update:
{::nomarkdown}$B_t = B_{t-1} + \alpha\mathbf{u}\mathbf{u}^\top${:/nomarkdown}, where {::nomarkdown}$\alpha \mathbf{u}\mathbf{u}^\top${:/nomarkdown} is a symmetric rank-1 correction which we can try to derive a formula for.

Let's plug that into the secant equation {::nomarkdown}$B_t\mathbf{s_t} = \mathbf{y_t}${:/nomarkdown}, replacing {::nomarkdown}$B_t${:/nomarkdown} with {::nomarkdown}$B_{t-1} + \alpha\mathbf{u}\mathbf{u}^\top${:/nomarkdown}. I leave the entire derivation there, but basically you can express {::nomarkdown}$\alpha\mathbf{u}\mathbf{u}^\top${:/nomarkdown} through {::nomarkdown}$B_t${:/nomarkdown}, {::nomarkdown}$\mathbf{s_t}${:/nomarkdown} and {::nomarkdown}$\mathbf{y_t}${:/nomarkdown}.

{::nomarkdown}$$(B_{t-1} + \alpha\mathbf{u}\mathbf{u}^\top)\mathbf{s}_t = \mathbf{y}_t$${:/nomarkdown}

{::nomarkdown}$$\text{(1) unpack brackets:}\\\\
\mathbf{y}_t = B_{t-1}\mathbf{s}_t + \alpha\mathbf{u}\mathbf{u}^\top \mathbf{s}_t$${:/nomarkdown}

{::nomarkdown}$$\text{(2) rearrange:}\\\\
\alpha\mathbf{u}\mathbf{u}^\top \mathbf{s}_t = \mathbf{y}_t - B_{t-1}\mathbf{s}_t$${:/nomarkdown}

{::nomarkdown}$$\text{(3) transpose both sides:}\\\\
(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)^\top = \alpha \mathbf{s}_t^\top\mathbf{u}\mathbf{u}^\top$${:/nomarkdown}

{::nomarkdown}$$\text{(4) multiply (2) by (3):}\\\\
(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)^\top = (\alpha \mathbf{u} \mathbf{u}^\top \mathbf{s}_t) (\alpha \mathbf{s}_t^\top \mathbf{u} \mathbf{u}^\top) $${:/nomarkdown}

{::nomarkdown}$$\text{(5) rearrange right-hand side:}\\\\
(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)^\top = \alpha^2 \cdot (\mathbf{u}^\top \mathbf{s}_t) \cdot (\mathbf{s_t}^\top\mathbf{u}) \cdot \mathbf{u} \mathbf{u}^\top = \alpha^2 (\mathbf{u}^\top \mathbf{s}_t)^2 \mathbf{u} \mathbf{u}^\top
$${:/nomarkdown}

{::nomarkdown}$$\text{(6) divide by } \alpha \cdot (\mathbf{u}^\top \mathbf{s}_t)^2:\\\\
\alpha \mathbf{u} \mathbf{u}^\top = \frac{(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)^\top}{\alpha \cdot (\mathbf{u}^\top \mathbf{s}_t)^2}
$${:/nomarkdown}

Second part of the derivation is to get rid of {::nomarkdown}$(\mathbf{u}^\top \mathbf{s}_t)^2${:/nomarkdown} in the denominator:

{::nomarkdown}$$(B_{t-1} + \alpha\mathbf{u}\mathbf{u}^\top)\mathbf{s}_t = \mathbf{y_t}$${:/nomarkdown}

{::nomarkdown}$$\text{(7) left-multiply by } {\mathbf{s}_t^\top}:\\\\
\mathbf{s}_t^\top\mathbf{y}_t = \mathbf{s}_t^\top B_{t-1} \mathbf{s_t} + \mathbf{s}_t^\top (\alpha\mathbf{u}\mathbf{u}^\top \mathbf{s}_t)
$${:/nomarkdown}
{::nomarkdown}$$
\mathbf{s}_t^\top\mathbf{y}_t = \mathbf{s}_t^\top B_{t-1} \mathbf{s_t} + \alpha(\mathbf{s_t}^\top\mathbf{u})(\mathbf{u}^\top \mathbf{s}_t)
$${:/nomarkdown}
{::nomarkdown}$$
\mathbf{s}_t^\top\mathbf{y}_t = \mathbf{s}_t^\top B_{t-1} \mathbf{s_t} + \alpha(\mathbf{u}^\top \mathbf{s}_t)^2
$${:/nomarkdown}
{::nomarkdown}$$\text{(8) rearrange:}\\\\
(\mathbf{u}^\top \mathbf{s}_t)^2 = \mathbf{s}_t^\top (\mathbf{y}_t - B_{t-1}\mathbf{s}_t)
$${:/nomarkdown}

Now we put (8) into (6) and we get SR1 correction formula:
{::nomarkdown}$$
\alpha \mathbf{u} \mathbf{u}^\top = \frac{(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)^\top}{\mathbf{s}_t^\top (\mathbf{y}_t - B_{t-1}\mathbf{s}_t)}
$${:/nomarkdown}

So, given {::nomarkdown}$B_{t-1}${:/nomarkdown}, {::nomarkdown}$\mathbf{s}_t${:/nomarkdown} and {::nomarkdown}$\mathbf{y}_t${:/nomarkdown}, the Hessian approximation is updated like this:
{::nomarkdown}$$
B_t \leftarrow B_{t-1} + \frac{(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)^\top}{\mathbf{s}_t^\top (\mathbf{y}_t - B_{t-1}\mathbf{s}_t)}
$${:/nomarkdown}

We can also do the same for Hessian inverse. {::nomarkdown}$B_t\mathbf{s}_t=\mathbf{y}_t${:/nomarkdown}, therefore {::nomarkdown}$B_t^{-1}\mathbf{y}_t=\mathbf{s}_t${:/nomarkdown}. We can repeat the entire derivation above, simply swapping {::nomarkdown}$\mathbf{s}_t${:/nomarkdown} and {::nomarkdown}$\mathbf{y}_t${:/nomarkdown}, and we get a formula which updates Hessian inverse approximation:
{::nomarkdown}$$
B_t^{-1} \leftarrow B_{t-1}^{-1} + \frac{(\mathbf{s}_t - B_{t-1}\mathbf{y}_t)(\mathbf{s}_t - B_{t-1}\mathbf{y}_t)^\top}{\mathbf{y}_t^\top (\mathbf{s}_t - B_{t-1}\mathbf{y}_t)}
$${:/nomarkdown}

Note that swapping {::nomarkdown}$\mathbf{s}_t${:/nomarkdown} and {::nomarkdown}$\mathbf{y}_t${:/nomarkdown} works for SR1, but it doesn't work for all formulas. For example BFGS and DFP methods are duals - if you swap {::nomarkdown}$\mathbf{s}_t${:/nomarkdown} and {::nomarkdown}$\mathbf{y}_t${:/nomarkdown} in BFGS formula for Hessian, you don't get BFGS formula for inverse Hessian, instead you get DFP formula for inverse Hessian, and vice-versa.

## Final algorithm

Now that we have a formula for updating Hessian or Hessian inverse approximation, let's see how it is applid to minimize a function.

#### Initialization

First we need to initialize the Hessian approximation {::nomarkdown}$B_0${:/nomarkdown}. Usually it is initialized to a scaled identity matrix. It can also be initialized to the true Hessian matrix or some guess of it.

A heuristic to determine scale of the initial identity matrix is described in [Wright, Stephen, and Jorge Nocedal. "Numerical optimization." Springer Science 35.67-68 (1999): 7.](https://www.math.uci.edu/~qnie/Publications/NumericalOptimization.pdf) on p.142, and it is the following:
{::nomarkdown}$$
B_1 = I \cdot \frac{\mathbf{y}_1^\top\mathbf{y}_1}{\mathbf{y}_1^\top \mathbf{s}_1}
$${:/nomarkdown}
and for Hessian inverse:
{::nomarkdown}$$
B_1^{-1} = I \cdot \frac{\mathbf{y}_1^\top\mathbf{s}_1}{\mathbf{y}_1^\top \mathbf{y}_1}
$${:/nomarkdown}

But for this formula we need {::nomarkdown}$\mathbf{s}${:/nomarkdown} and {::nomarkdown}$\mathbf{y}${:/nomarkdown} - differences between consequtive parameters and gradients, which are not available on the first step since we don't yet have previous parameters and gradients. So on first step we set {::nomarkdown}$B_0=I${:/nomarkdown}, meaning first step is just a gradient descent step, and on second step we obtain our first {::nomarkdown}$\mathbf{s_1}${:/nomarkdown} and {::nomarkdown}$\mathbf{y_1}${:/nomarkdown} and set {::nomarkdown}$B_1=I * \frac{\mathbf{y}_1^\top\mathbf{y}_1}{\mathbf{y}_1^\top \mathbf{s}_1}${:/nomarkdown}.

This heuristic is actually the [Barzilai–Borwein](https://en.wikipedia.org/wiki/Barzilai%E2%80%93Borwein_method) step size. Barzilai–Borwein method uses {::nomarkdown}$I \cdot \frac{\mathbf{y}_1^\top\mathbf{y}_1}{\mathbf{y}_1^\top \mathbf{s}_1}${:/nomarkdown} as the hessian approximation. This scaling corresponds to the least-squares solution to {::nomarkdown}$B\mathbf{s}=\mathbf{y}${:/nomarkdown} where {::nomarkdown}$B${:/nomarkdown} is constrained to be scaled identity; it is the only solution, so no least-change principle can be used here.


#### Performing optimization

There are multiple ways to perform optimization with a Quasi-Newton Hessian approximation, it can be used with a line search or a trust region. Using a fixed step size is less common as it isn't stable enough. I must note that SR1 that we derived above tends to be unstable with a line search, and is much more suitable for trust region. The most popular Quasi-Newton formula, BFGS, works well with both line search and trust region. Here we will review the line search approach because it is simpler.

We have {::nomarkdown}$B_{t-1}${:/nomarkdown} - current Hessian approximation. The algorithm for most Quasi-Newton methods is very similar - on time step {::nomarkdown}$t${:/nomarkdown} it is the following:

1. compute {::nomarkdown}$\mathbf{s}_t${:/nomarkdown} - difference between current and previous parameters:

{::nomarkdown}$$\mathbf{s}_t = \mathbf{x}_t - \mathbf{x}_{t-1}$${:/nomarkdown}

2. compute {::nomarkdown}$\mathbf{y}_t${:/nomarkdown} - difference between current and previous gradients:

{::nomarkdown}$$\mathbf{y}_t = \mathbf{g}(\mathbf{x}_t) - \mathbf{g}(\mathbf{x}_{t-1})$${:/nomarkdown}

3. Update Hessian approximation (or Hessian inverse approximation), using some Quasi-Newton formula. For example we can use SR1 formula:

{::nomarkdown}$$
B_t \leftarrow B_{t-1} + \frac{(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)^\top}{\mathbf{s}_t^\top (\mathbf{y}_t - B_{t-1}\mathbf{s}_t)}
$${:/nomarkdown}

5. Determine step size {::nomarkdown}$\gamma_t${:/nomarkdown}, usually via a line search along the Quasi-Newton direction {::nomarkdown}$B_t^{-1} \mathbf{g}(\mathbf{x}_t)${:/nomarkdown}:

{::nomarkdown}$$\gamma_t = \argmin_{\gamma} f(x_t - \gamma  B_t^{-1} \mathbf{g}(\mathbf{x}_t))$${:/nomarkdown}

6. Update the parameters:

{::nomarkdown}$$x_{t+1} \leftarrow x_t - \gamma_t  B_t^{-1} \mathbf{g}(\mathbf{x}_t)$${:/nomarkdown}

## QN in practice

In practice the most widely used method is BFGS, simply because it tends to perform well. SR1 formula [converges to true hessian faster than BFGS](https://arxiv.org/pdf/2002.00657), but it is less stable. There are countless other Quasi-Newton methods, some may work better for specific problems.

A notable disadvantage of Quasi-Newton methods is that for a problem with {::nomarkdown}$n${:/nomarkdown} variables they have to store an {::nomarkdown}$n \times n${:/nomarkdown} matrix {::nomarkdown}$B${:/nomarkdown}, so you wouldn't use them for problems over 10,000 variables. There are memory efficient variants, most notably L-BFGS which only stores past {::nomarkdown}$k${:/nomarkdown} pairs of {::nomarkdown}$\mathbf{s}${:/nomarkdown} and {::nomarkdown}$\mathbf{y}${:/nomarkdown}, where {::nomarkdown}$k${:/nomarkdown} is the history size hyperparameter.

Quasi-Newton methods perform best on smooth, not necessarily convex functions. However BFGS has been shown to be [robust for non-smooth optimization](https://cs.nyu.edu/~overton/papers/pdffiles/bfgs_inexactLS.pdf) even where true hessian is zero everywhere.

L-BFGS is the [standard method for style transfer](https://arxiv.org/pdf/1505.07376) - because it has the fastest convergence and requires no tuning compared to tuned Adam, SOAP, etc.
![Style Transfer benchmark](/assets/quasi-newton/style-transfer.png)
_Style Transfer benchmark_

It has also been [widely used for Physics-inspired neural networks](https://arxiv.org/html/2405.04230v3), although recently [SOAP has been shown to outperform it](https://arxiv.org/html/2502.00604v1).
![PINN benchmark](/assets/quasi-newton/pinn.png)
_PINN benchmark_

L-BFGS is also the standard method for logistic regression, for example it is the [default solver in sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).

Quasi-Newton methods have not had as much success in deep learning, mainly due to the effectiveness of mini-batching, which of course introduces stochasticity and makes QN methods ineffective. There have been many attempts at making QN work with stochastic optimization, but none seemed to have stuck around. But in some cases it may be desired to obtain a perfect fit with a shallow neural network, here Quasi-Newton methods will often outperform Adam/SOAP.

I have observed particularly impressive convergence of Shor's r-algorithm on fitting a single layer ReLU network. ShorR is not actually a Quasi-Newton method because the update to the curvature matrix doesn't satisfy the secant equation, but it is otherwise very similar. Take a look at [this notebook](https://github.com/inikishev/inikishev/blob/09aa4b175a7dc1c80d5313afeaa257e5f2587a72/notebooks/Baarle-Hertog%20border%20fitting/1k%20ShorR%20large%20batch%201000%20epochs.ipynb), this is a solution to the border fitting example from [Welch Labs video](https://www.youtube.com/watch?v=qx7hirqgfuU). I tried so many methods and nothing comes even close to ShorR.

To conclude this, I have decided to run 25 QN methods on Rosenbrock's function. Note that this doesn't necessarily represent their performance on larger problems!
![QN on rosenbrock](/assets/quasi-newton/rosen.png)
_25 Quasi-Newton methods on Rosenbrock_
