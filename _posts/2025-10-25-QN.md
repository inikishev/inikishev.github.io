---
title: Simple explanation of Quasi-Newton methods
date: 2025-10-25 00:00:00 +0300
categories: [Optimization, Quasi-Newton]
tags: [optimization, quasi-newton, sr1]     # TAG names should always be lowercase
description: Simple explanation of the secant equation and how quasi-newton methods use it.
math: true
---

## Preface

At time step {::nomarkdown}$t${:/nomarkdown} the parameters will be denoted as {::nomarkdown}$\mathbf{x}_t${:/nomarkdown}; gradient at those parameters as {::nomarkdown}$\mathbf{g}(\mathbf{x}_t)${:/nomarkdown} and Hessian as {::nomarkdown}$H(\mathbf{x}_t)${:/nomarkdown}.

For this article I assume the reader knows what gradient and Hessian are, and is familiar with Newton's method for optimization, which in the most basic form performs the following update:
{::nomarkdown}$$
\mathbf{x}_{t+1} \leftarrow \mathbf{x}_t - H(\mathbf{x}_t)^{-1}\mathbf{g}(\mathbf{x}_t)
$${:/nomarkdown}

## Introduction

Quasi-newton methods approximate the Hessian or Hessian inverse using only gradients. Let's see how they do that!

## Secant equation

Secant equation is a core concept in Quasi-Newton methods.

Suppose we have Hessian {::nomarkdown}$H${:/nomarkdown}, some vector {::nomarkdown}$\mathbf{s}${:/nomarkdown}, and we compute a Hessian-vector product with it and call it {::nomarkdown}$\mathbf{y}${:/nomarkdown}. So we get:
{::nomarkdown}$$
H\mathbf{s} = \mathbf{y}
$${:/nomarkdown}
This is the secant equation. If we had {::nomarkdown}$\mathbf{s}${:/nomarkdown} and {::nomarkdown}$\mathbf{y}${:/nomarkdown}, we would know that the Hessian must satisfy {::nomarkdown}$H\mathbf{s} = \mathbf{y}${:/nomarkdown}.

It might seem that in order to obtain {::nomarkdown}$\mathbf{y}${:/nomarkdown}, one needs the Hessian to compute {::nomarkdown}$H\mathbf{s}${:/nomarkdown}. But there is a simple way to get {::nomarkdown}$\mathbf{s}${:/nomarkdown} and {::nomarkdown}$\mathbf{y}${:/nomarkdown} basically for free without knowing the Hessian.

## Finite-difference Hessian-vector products

If we compute difference between consequtive parameters and assign it to {::nomarkdown}$\mathbf{s}_t${:/nomarkdown}, and assign difference between consequtive gradients to {::nomarkdown}$\mathbf{y}_t${:/nomarkdown}, then {::nomarkdown}$\mathbf{y}_t${:/nomarkdown} approximates Hessian-vector product with {::nomarkdown}$\mathbf{s}_t${:/nomarkdown}:
{::nomarkdown}$$
\mathbf{s}_t = \mathbf{x}_t - \mathbf{x}_{t-1}
$${:/nomarkdown}
{::nomarkdown}$$
\mathbf{y}_t = \mathbf{g}(\mathbf{x}_t) - \mathbf{g}(\mathbf{x}_{t-1})
$${:/nomarkdown}
{::nomarkdown}$$
\text{then: } H(\mathbf{x}_t) \mathbf{s_t} \approx \mathbf{y}_t
$${:/nomarkdown}

To understand why, we can look at the formula of a [finite-differerence approximation to a Hessian-vector product](https://justindomke.wordpress.com/2009/01/17/Hessian-vector-products/).

 A Hessian-vector product, such as {::nomarkdown}$H(\mathbf{x})\mathbf{s}${:/nomarkdown}, can be approximated using this (backward finite difference) formula:
{::nomarkdown}$$
H(\mathbf{x})\mathbf{s} \approx \frac{\mathbf{g}(\mathbf{x}) - \mathbf{g}(\mathbf{x} - r\mathbf{s})}{r}
$${:/nomarkdown}

Here {::nomarkdown}$\mathbf{x}${:/nomarkdown} is the parameters where Hessian-vector product is approximated, and {::nomarkdown}$r${:/nomarkdown} is a small constant. With infinite precision the approximation becomes exact in the limit {::nomarkdown}$r \rightarrow 0${:/nomarkdown}.

Now let's set {::nomarkdown}$r${:/nomarkdown} to 1 and replace {::nomarkdown}$\mathbf{s}${:/nomarkdown} with {::nomarkdown}$\mathbf{x}_t - \mathbf{x}_{t-1}${:/nomarkdown}:
{::nomarkdown}$$
H(\mathbf{x}_t) \mathbf{s}_t \approx \frac{\mathbf{g}(\mathbf{x}_t) - \mathbf{g}(\mathbf{x}_t - 1 (\mathbf{x}_t - \mathbf{x}_{t-1}))}{1} = \mathbf{g}(\mathbf{x}_t) - \mathbf{g}(\mathbf{x}_{t-1}) = \mathbf{y}_t
$${:/nomarkdown}
so:
{::nomarkdown}$$
H(\mathbf{x}_t) \mathbf{s}_t \approx \mathbf{y}_t
$${:/nomarkdown}

Of course by setting {::nomarkdown}$r${:/nomarkdown} to 1, it seems that the approximation would be crude, but in practice it works exceptionally well in Quasi-Newton methods.

## Least-change principle

Quasi-Newton methods maintain a hessian approximation {::nomarkdown}$B${:/nomarkdown} (or it's inverse) and refine it on each iteration.

Suppose we have current Hessian approximation {::nomarkdown}$B_{t-1}${:/nomarkdown}, and we obtain {::nomarkdown}$\mathbf{s}_t = \mathbf{x}_t - \mathbf{x}_{t-1}${:/nomarkdown} and {::nomarkdown}$\mathbf{y}_t = \mathbf{g}(\mathbf{x}_t) - \mathbf{g}(\mathbf{x}_{t-1})${:/nomarkdown}. We know that {::nomarkdown}$H(\mathbf{x}_t) \mathbf{s}_t \approx \mathbf{y}_t${:/nomarkdown}, therefore we want our new refined hessian approximation {::nomarkdown}$B_t${:/nomarkdown} to satisfy {::nomarkdown}$B_t \mathbf{s}_t = \mathbf{y}_t${:/nomarkdown}. However on it's own this isn't too useful as there are infinitely many possible {::nomarkdown}$B_t${:/nomarkdown} that satisfy {::nomarkdown}$B_t \mathbf{s}_t = \mathbf{y}_t${:/nomarkdown}.

The idea of the least-change principle is to pick {::nomarkdown}$B_t${:/nomarkdown} such that it is close to {::nomarkdown}$B_{t-1}${:/nomarkdown}, preferably as close as possible. Since {::nomarkdown}$B_{t-1}${:/nomarkdown} might've accumulated a lot of curvature information from previous iterations, we don't want to throw that information away - keeping {::nomarkdown}$B_t${:/nomarkdown} close to {::nomarkdown}$B_{t-1}${:/nomarkdown} makes sure as little of that information is lost as possible. "Close" can be interpreted in many different ways, leading to different Quasi-Newton formulas.

Least-change principle is a heuristic (to my knowledge), but it works exceptionally well in practice and converges to true Hessian. There are multiple interpretations of why it works, for example:

- *Lukšan, L., & Spedicato, E. (2000). Variable metric methods for unconstrained optimization and nonlinear least squares. Journal of Computational and Applied Mathematics, 124(1-2), 61-95.*:

  > Roughly speaking, the least-change principle guarantees that as much information from previous iterations as possible is saved while the quasi-Newton condition brings new information because it is satisfied by matrix.

- *[Greenstadt, J. (1970). Variations on variable-metric methods.(With discussion). Mathematics of Computation, 24(109), 1-22.](https://web.archive.org/web/20170817130143id_/https://www.ams.org/journals/mcom/1970-24-109/S0025-5718-1970-0258248-4/S0025-5718-1970-0258248-4.pdf)*:

  > Let us ask for the “best” correction in some sense. There are many possible choices to make, but a good one is to ask for the smallest correction, in the sense of some norm. To a certain extent, this would tend to keep the elements of [{::nomarkdown}$B^{−1}_k${:/nomarkdown} ] from growing too large, which might cause an undesirable instability.

## Deriving a Quasi-Newton method

Now let's derive a Quasi-Newton method. We will derive the Symmetric Rank 1 (SR1) method because it is simple.

What we have:

- current Hessian approximation {::nomarkdown}$B_{t-1}${:/nomarkdown};
- {::nomarkdown}$\mathbf{s}_t = \mathbf{x}_t - \mathbf{x}_{t-1}${:/nomarkdown}
- {::nomarkdown}$\mathbf{y}_t = \mathbf{g}(\mathbf{x}_t) - \mathbf{g}(\mathbf{x}_{t-1})${:/nomarkdown};

What we want:

- {::nomarkdown}$B_t${:/nomarkdown} that satisfies the secant equation {::nomarkdown}$B_t\mathbf{s}_t=\mathbf{y}_t${:/nomarkdown}
- we want {::nomarkdown}$B_t${:/nomarkdown} to be close to {::nomarkdown}$B_{t-1}${:/nomarkdown} in some sense, so that little information is lost from {::nomarkdown}$B_t${:/nomarkdown}
- there are other useful constraints one can impose on {::nomarkdown}$B_t${:/nomarkdown}:
  - usually {::nomarkdown}$B_t${:/nomarkdown} is constrained to be symmetric: {::nomarkdown}$B_t=B_t^{\text{T}}${:/nomarkdown}.
  - {::nomarkdown}$B_t${:/nomarkdown} is often constrained to be positive-definite, however the SR1 formula does not do that.

A simple way to keep {::nomarkdown}$B_t${:/nomarkdown} symmetric and not too far from {::nomarkdown}$B_{t-1}${:/nomarkdown} is to perform a symmetric rank-1 update:
{::nomarkdown}$B_t = B_{t-1} + \alpha\mathbf{u}\mathbf{u}^{\text{T}}${:/nomarkdown}, where {::nomarkdown}$\alpha \mathbf{u}\mathbf{u}^{\text{T}}${:/nomarkdown} is a symmetric rank-1 correction which we can try to derive a formula for.

Let's plug that into the secant equation {::nomarkdown}$B_t\mathbf{s_t} = \mathbf{y_t}${:/nomarkdown}, replacing {::nomarkdown}$B_t${:/nomarkdown} with {::nomarkdown}$B_{t-1} + \alpha\mathbf{u}\mathbf{u}^{\text{T}}${:/nomarkdown}. I leave the entire derivation there, but basically there is a single unique solution.

{::nomarkdown}$$(B_{t-1} + \alpha\mathbf{u}\mathbf{u}^{\text{T}})\mathbf{s}_t = \mathbf{y}_t$${:/nomarkdown}

{::nomarkdown}$$
\begin{gather*}
\text{(1) unpack brackets:} \\
\mathbf{y}_t = B_{t-1}\mathbf{s}_t + \alpha\mathbf{u}\mathbf{u}^{\text{T}} \mathbf{s}_t
\end{gather*}
$${:/nomarkdown}

{::nomarkdown}$$
\begin{gather*}
\text{(2) rearrange:}\\
\alpha\mathbf{u}\mathbf{u}^{\text{T}} \mathbf{s}_t = \mathbf{y}_t - B_{t-1}\mathbf{s}_t
\end{gather*}
$${:/nomarkdown}

{::nomarkdown}$$
\begin{gather*}
\text{(3) transpose both sides:}\\
(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)^{\text{T}} = \alpha \mathbf{s}_t^{\text{T}}\mathbf{u}\mathbf{u}^{\text{T}}
\end{gather*}
$${:/nomarkdown}

{::nomarkdown}$$
\begin{gather*}
\text{(4) multiply (2) by (3):}\\
(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)^{\text{T}} = (\alpha \mathbf{u} \mathbf{u}^{\text{T}} \mathbf{s}_t) (\alpha \mathbf{s}_t^{\text{T}} \mathbf{u} \mathbf{u}^{\text{T}})
\end{gather*}
$${:/nomarkdown}

{::nomarkdown}$$
\begin{gather*}
\text{(5) rearrange right-hand side:}\\
(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)^{\text{T}} = \alpha^2 \cdot (\mathbf{u}^{\text{T}} \mathbf{s}_t) \cdot (\mathbf{s_t}^{\text{T}}\mathbf{u}) \cdot \mathbf{u} \mathbf{u}^{\text{T}} = \alpha^2 (\mathbf{u}^{\text{T}} \mathbf{s}_t)^2 \mathbf{u} \mathbf{u}^{\text{T}}
\end{gather*}
$${:/nomarkdown}

{::nomarkdown}$$
\begin{gather*}
\text{(6) divide by } \alpha \cdot (\mathbf{u}^{\text{T}} \mathbf{s}_t)^2:\\
\alpha \mathbf{u} \mathbf{u}^{\text{T}} = \frac{(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)^{\text{T}}}{\alpha \cdot (\mathbf{u}^{\text{T}} \mathbf{s}_t)^2}
\end{gather*}
$${:/nomarkdown}

Second part of the derivation is to get rid of {::nomarkdown}$(\mathbf{u}^{\text{T}} \mathbf{s}_t)^2${:/nomarkdown} in the denominator:

{::nomarkdown}$$(B_{t-1} + \alpha\mathbf{u}\mathbf{u}^{\text{T}})\mathbf{s}_t = \mathbf{y_t}$${:/nomarkdown}

{::nomarkdown}$$
\begin{gather*}
\text{(7) left-multiply by } {\mathbf{s}_t^{\text{T}}}:\\
\mathbf{s}_t^{\text{T}}\mathbf{y}_t = \mathbf{s}_t^{\text{T}} B_{t-1} \mathbf{s_t} + \mathbf{s}_t^{\text{T}} (\alpha\mathbf{u}\mathbf{u}^{\text{T}} \mathbf{s}_t)
\end{gather*}
$${:/nomarkdown}

{::nomarkdown}$$
\mathbf{s}_t^{\text{T}}\mathbf{y}_t = \mathbf{s}_t^{\text{T}} B_{t-1} \mathbf{s_t} + \alpha(\mathbf{s_t}^{\text{T}}\mathbf{u})(\mathbf{u}^{\text{T}} \mathbf{s}_t)
$${:/nomarkdown}

{::nomarkdown}$$
\mathbf{s}_t^{\text{T}}\mathbf{y}_t = \mathbf{s}_t^{\text{T}} B_{t-1} \mathbf{s_t} + \alpha(\mathbf{u}^{\text{T}} \mathbf{s}_t)^2
$${:/nomarkdown}

{::nomarkdown}$$
\begin{gather*}\text{(8) rearrange:}\\
(\mathbf{u}^{\text{T}} \mathbf{s}_t)^2 = \mathbf{s}_t^{\text{T}} (\mathbf{y}_t - B_{t-1}\mathbf{s}_t)
\end{gather*}
$${:/nomarkdown}

Now we put (8) into (6) and we get SR1 correction formula:
{::nomarkdown}$$
\alpha \mathbf{u} \mathbf{u}^{\text{T}} = \frac{(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)^{\text{T}}}{\mathbf{s}_t^{\text{T}} (\mathbf{y}_t - B_{t-1}\mathbf{s}_t)}
$${:/nomarkdown}

So, given {::nomarkdown}$B_{t-1}${:/nomarkdown}, {::nomarkdown}$\mathbf{s}_t${:/nomarkdown} and {::nomarkdown}$\mathbf{y}_t${:/nomarkdown}, the Hessian approximation is updated like this:
{::nomarkdown}$$
B_t \leftarrow B_{t-1} + \frac{(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)(\mathbf{y}_t - B_{t-1}\mathbf{s}_t)^{\text{T}}}{\mathbf{s}_t^{\text{T}} (\mathbf{y}_t - B_{t-1}\mathbf{s}_t)}
$${:/nomarkdown}

We can also do the same for Hessian inverse. {::nomarkdown}$B_t\mathbf{s}_t=\mathbf{y}_t${:/nomarkdown}, therefore {::nomarkdown}$B_t^{-1}\mathbf{y}_t=\mathbf{s}_t${:/nomarkdown}. We can repeat the entire derivation above, simply swapping {::nomarkdown}$\mathbf{s}_t${:/nomarkdown} and {::nomarkdown}$\mathbf{y}_t${:/nomarkdown}, and we get a formula which updates Hessian inverse approximation:
{::nomarkdown}$$
B_t^{-1} \leftarrow B_{t-1}^{-1} + \frac{(\mathbf{s}_t - B_{t-1}^{-1}\mathbf{y}_t)(\mathbf{s}_t - B_{t-1}^{-1}\mathbf{y}_t)^{\text{T}}}{\mathbf{y}_t^{\text{T}} (\mathbf{s}_t - B_{t-1}^{-1}\mathbf{y}_t)}
$${:/nomarkdown}

Note that swapping {::nomarkdown}$\mathbf{s}_t${:/nomarkdown} and {::nomarkdown}$\mathbf{y}_t${:/nomarkdown} works for SR1, but it doesn't work for all formulas. For example BFGS and DFP methods are duals - if you swap {::nomarkdown}$\mathbf{s}_t${:/nomarkdown} and {::nomarkdown}$\mathbf{y}_t${:/nomarkdown} in BFGS formula for Hessian, you don't get BFGS formula for inverse Hessian, instead you get DFP formula for inverse Hessian, and vice-versa.

## Final algorithm

Now that we have a formula for updating Hessian or Hessian inverse approximation, let's see how it is applid to minimize a function.

#### Initialization

First we need to initialize the Hessian approximation {::nomarkdown}$B_0${:/nomarkdown}. Usually it is initialized to a scaled identity matrix. It can also be initialized to the true Hessian matrix or some guess of it.

A heuristic to determine scale of the initial identity matrix is described in [Wright, Stephen, and Jorge Nocedal. "Numerical optimization." Springer Science 35.67-68 (1999): 7.](https://www.math.uci.edu/~qnie/Publications/NumericalOptimization.pdf) on p.142, and it is the following:
{::nomarkdown}$$
B_1 = I \cdot \frac{\mathbf{y}_1^{\text{T}}\mathbf{y}_1}{\mathbf{y}_1^{\text{T}} \mathbf{s}_1}
$${:/nomarkdown}
and for Hessian inverse:
{::nomarkdown}$$
B_1^{-1} = I \cdot \frac{\mathbf{y}_1^{\text{T}}\mathbf{s}_1}{\mathbf{y}_1^{\text{T}} \mathbf{y}_1}
$${:/nomarkdown}

But for this formula we need {::nomarkdown}$\mathbf{s}${:/nomarkdown} and {::nomarkdown}$\mathbf{y}${:/nomarkdown} - differences between consequtive parameters and gradients, which are not available on the first step since we don't yet have previous parameters and gradients. So on first step we set {::nomarkdown}$B_0=I${:/nomarkdown}, meaning first step is just a gradient descent step, and on second step we obtain our first {::nomarkdown}$\mathbf{s_1}${:/nomarkdown} and {::nomarkdown}$\mathbf{y_1}${:/nomarkdown} and set {::nomarkdown}$B_1=I * \frac{\mathbf{y}_1^{\text{T}}\mathbf{y}_1}{\mathbf{y}_1^{\text{T}} \mathbf{s}_1}${:/nomarkdown}.

This heuristic is actually the [Barzilai–Borwein](https://en.wikipedia.org/wiki/Barzilai%E2%80%93Borwein_method) step size. Barzilai–Borwein method uses {::nomarkdown}$I \cdot \frac{\mathbf{y}_1^{\text{T}}\mathbf{y}_1}{\mathbf{y}_1^{\text{T}} \mathbf{s}_1}${:/nomarkdown} as the hessian approximation. This scaling corresponds to the least-squares solution to {::nomarkdown}$B^{-1}\mathbf{y}=\mathbf{s}${:/nomarkdown} where {::nomarkdown}$B^{-1}${:/nomarkdown} is constrained to be scaled identity; it is the only solution, so least-change principle can't be used here.

#### Performing optimization

There are multiple ways to perform optimization with a Quasi-Newton Hessian approximation, it can be used with a line search or a trust region. Using a fixed step size is less common as it isn't stable enough. I must note that SR1 that we derived above tends to be unstable with a line search, and is much more suitable for trust region. The most popular Quasi-Newton formula, BFGS, works well with both line search and trust region. Here we will review the line search approach because it is simpler.

The algorithm for most Quasi-Newton methods is very similar. We have {::nomarkdown}$B_{t-1}${:/nomarkdown} - current Hessian approximation, or {::nomarkdown}$B_{t-1}^{-1}${:/nomarkdown} - current hessian inverse approximation. On time step {::nomarkdown}$t${:/nomarkdown} it is the following:

{::nomarkdown}$1.${:/nomarkdown} compute {::nomarkdown}$\mathbf{s}_t${:/nomarkdown} - difference between current and previous parameters:

{::nomarkdown}$$\mathbf{s}_t = \mathbf{x}_t - \mathbf{x}_{t-1}$${:/nomarkdown}

{::nomarkdown}$2.${:/nomarkdown} compute {::nomarkdown}$\mathbf{y}_t${:/nomarkdown} - difference between current and previous gradients:

{::nomarkdown}$$\mathbf{y}_t = \mathbf{g}(\mathbf{x}_t) - \mathbf{g}(\mathbf{x}_{t-1})$${:/nomarkdown}

{::nomarkdown}$3.${:/nomarkdown} Update Hessian approximation (or Hessian inverse approximation), using some Quasi-Newton formula. Maintaining a hessian inverse approximation is more efficient since we will need to compute {::nomarkdown}$B_t^{-1} \mathbf{g}(\mathbf{x}_t)${:/nomarkdown}. We could use the SR1 formula derived above, but since it doesn't work well with line searches, let's use the BFGS formula:

{::nomarkdown}$$
B_{t}^{-1} \leftarrow B_{t-1}^{-1} + \frac{(\mathbf{s}_t^T \mathbf{y}_t + \mathbf{y}_t^T B_{t-1}^{-1} \mathbf{y}_t) \mathbf{s}_t \mathbf{s}_t^T}{(\mathbf{s}_t^T \mathbf{y}_t)^2} - \frac{B_{t-1}^{-1} \mathbf{y}_t \mathbf{s}_t^T + \mathbf{s}_t \mathbf{y}_t^T B_{t-1}^{-1}}{\mathbf{s}_t^T \mathbf{y}_t}
$${:/nomarkdown}

{::nomarkdown}$4.${:/nomarkdown} Determine step size {::nomarkdown}$\gamma_t${:/nomarkdown}, usually via a line search along the Quasi-Newton direction {::nomarkdown}$B_t^{-1} \mathbf{g}(\mathbf{x}_t)${:/nomarkdown}:

{::nomarkdown}$$\gamma_t = \underset{\gamma}{\text{argmin}} f(x_t - \gamma  B_t^{-1} \mathbf{g}(\mathbf{x}_t))$${:/nomarkdown}

{::nomarkdown}$5.${:/nomarkdown} Update the parameters:

{::nomarkdown}$$x_{t+1} \leftarrow x_t - \gamma_t  B_t^{-1} \mathbf{g}(\mathbf{x}_t)$${:/nomarkdown}

## Further developments

I will now briefly outline some important modifications to Quasi-Newton (QN) methods. This is not an exhaustive list and I might update it in the future.

### Limited-memory

Limited-memory variants are probably the most important and widely used modifications of QN methods. For a problem with {::nomarkdown}$n${:/nomarkdown} variables standard QN uses at least {::nomarkdown}$n^2${:/nomarkdown} memory to store {::nomarkdown}$B${:/nomarkdown} or {::nomarkdown}$B^{-1}${:/nomarkdown}, so it can't be applied to large scale problems.

Limited-memory variants store {::nomarkdown}$B${:/nomarkdown} more efficiently and usually only consider a limited history of past {::nomarkdown}$\mathbf{s}_t${:/nomarkdown} and {::nomarkdown}$\mathbf{y}_t${:/nomarkdown} pairs.

Let's say we've initialized {::nomarkdown}$B${:/nomarkdown} to scaled identity and performed {::nomarkdown}$k${:/nomarkdown} updates with the BFGS formula, where each update is a rank-2 correction. After {::nomarkdown}$k${:/nomarkdown} rank-2 updates {::nomarkdown}$B${:/nomarkdown} it becomes scaled identity plus at most rank-{::nomarkdown}$2k${:/nomarkdown} symmetric matrix. This can be represented as {::nomarkdown}$B = \alpha I + U U^T${:/nomarkdown}, where {::nomarkdown}$U${:/nomarkdown} is a {::nomarkdown}$n \times k${:/nomarkdown} matrix. This representation is called compact representation and only requires {::nomarkdown}$n \times 2k${:/nomarkdown} memory.

Alternatively it is possible to store past {::nomarkdown}$k${:/nomarkdown} pairs of {::nomarkdown}$\mathbf{s}_t${:/nomarkdown} and {::nomarkdown}$\mathbf{y}_t${:/nomarkdown}, and compute {::nomarkdown}$B^{-1}\mathbf{g}(\mathbf{x}_t)${:/nomarkdown} via [two loop recursion](https://scispace.com/pdf/on-the-limited-memory-bfgs-method-for-large-scale-5f8hu2qim3.pdf), never explicitly forming {::nomarkdown}$B^{-1}${:/nomarkdown}, this also only requires {::nomarkdown}$n \times 2k${:/nomarkdown} memory. This is used, for example, by the L-BFGS method, which is probably the most widely used QN method to date.

### Diagonal QN

Diagonal QN methods store {::nomarkdown}$B${:/nomarkdown} as a diagonal matrix, therefore they only require {::nomarkdown}$n${:/nomarkdown} memory, and no matrix multiplications or linear solves.

Diagonal QN can't use the secant equation. If we take the secant equation {::nomarkdown}$B\mathbf{s}=\mathbf{y}${:/nomarkdown} and constrain {::nomarkdown}$B${:/nomarkdown} to be diagonal, the only solution is {::nomarkdown}$B = \mathbf{y}/\mathbf{s}${:/nomarkdown}, i.e. element-wise division of {::nomarkdown}$\mathbf{y}${:/nomarkdown} by {::nomarkdown}$\mathbf{s}${:/nomarkdown}. This is extremely unstable and basically doesn't work.

Instead diagonal QN use other equations, such as [weak secant equation](https://githubharald.github.io/quasi_cauchy.html#:~:text=This%20article%20contains%20some%20notes,y%20s%20T%20%E2%8B%85%20s%20.), [weak quasi-cauchy equations](https://www.math.uwaterloo.ca/~hwolkowi/henry/reports/cauchy.pdf), etc.

Current diagonal QN methods haven't seen widespread adoption, probably because they converge slower than limited-memory, but they are also even cheaper so they might have some use cases.

### Randomized/Greedy QN

QN methods take {::nomarkdown}$\mathbf{s}_t${:/nomarkdown} to be difference between consequtive parameters so that we have easy access to {::nomarkdown}$\mathbf{y}_t = H(\mathbf{x}_t)\mathbf{s}_t \approx \mathbf{g}(\mathbf{x}_t) - \mathbf{g}(\mathbf{x}_{t-1})${:/nomarkdown} approximation, but other choices of {::nomarkdown}$\mathbf{s}_t${:/nomarkdown} are possible. The main disadvantage of the approximation is that it depends on difference between consecutive gradients, which is too noisy when objective function is stochastic. A simple alternative is to sample a random (or some other) vector {::nomarkdown}$\mathbf{s}${:/nomarkdown}, and explicitly compute {::nomarkdown}$\mathbf{y}${:/nomarkdown} as hessian-vector product with {::nomarkdown}$\mathbf{s}${:/nomarkdown}, either via [finite-difference formula](https://justindomke.wordpress.com/2009/01/17/hessian-vector-products/) or [Pearlmutter's trick](https://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf). This is, for example, used in [Stochastic Quasi-Newton method (SQN)](https://arxiv.org/pdf/1401.7020).

Moreover it is possible to choose {::nomarkdown}$\mathbf{s}${:/nomarkdown} such that hessian-vector product with it gives us as much info about the hessian as possible, doing so is called [greedy quasi-Newton update](https://arxiv.org/pdf/2002.00657).

## Notes

For some diabolical reason in Quasi-Newton literature hessian inverse approximation is often denoted as {::nomarkdown}$H${:/nomarkdown}. To avoid confusing it with hessian I wrote it as {::nomarkdown}$B^{-1}${:/nomarkdown}. This is also not perfect but it's also used in some QN literature and I think it's less confusing.
